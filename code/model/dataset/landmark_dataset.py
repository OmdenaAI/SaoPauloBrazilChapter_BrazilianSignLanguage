from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import torch
from model.utils.utils import load_config, load_obj
from typing import Dict, List, Union, Callable, Tuple
from model.utils.path_utils import get_data_paths
from functools import partial
import os
from omegaconf import DictConfig
import os
from . import frame_sampling
from model.features.feature_processor import FeatureProcessor
import time

def uniform_intervals(start: int, end: int, interval: int):
    return list(range(start, end + 1, interval))


def random_timestamps(start: int, end: int, interval: int):
    return sorted(np.random.randint(start, end + 1, size=interval).tolist())


def uniform_timestamps(start: int, end: int, interval: int) -> List[int]:
    return list(np.linspace(start, end, num=interval, dtype=int))


INTERVAL_FUNCTIONS = {
    "uniform_intervals": uniform_intervals,
    "random_timestamps": random_timestamps,
    "uniform_timestamps": uniform_timestamps,
}


def select_frame_indices_by_time_interval(
    timestamps_ms: List[int], 
    interval_fn: Callable[[int, int], List[int]],
    min_frames_between_samples: int = None
) -> List[int]:
    """
    Selects frame indices based on desired timestamps generated by `interval_fn`.

    Parameters:
    -----------
    timestamps_ms : List[int]
        Sorted list of frame timestamps in milliseconds.
    interval_fn : Callable
        A function that takes (start_time, end_time) and returns a list of desired timestamps (ms).
    min_frames_between_samples : int, optional
        Minimum number of frames required between samples. Used to validate sampling.

    Returns:
    --------
    List[int] : Indices of frames closest to the desired timestamps.
    """
    timestamps = np.array(timestamps_ms)
    start_time, end_time = timestamps[0], timestamps[-1]
    target_times = interval_fn(int(start_time), int(end_time))

    selected_indices = []
    for target in target_times:
        closest_idx = np.argmin(np.abs(timestamps - target))
        
        # Check if this index maintains minimum frame separation
        if min_frames_between_samples is not None and selected_indices:
            if closest_idx - selected_indices[-1] < min_frames_between_samples:
                continue
                
        if not selected_indices or closest_idx != selected_indices[-1]:  # avoid duplicates
            selected_indices.append(closest_idx)

    return selected_indices


class LandmarkFeatureTorchJoiner:
    def forward(self, landmark_features: Dict):
        feature_vector = []
        for landmark_type in landmark_features.keys():
            feature_vector.extend(landmark_features[landmark_type])
        return torch.tensor(feature_vector, dtype=torch.float)


class LandmarkDataset(Dataset):
    def __init__(
        self,
        dataset_config: Union[str, Dict, DictConfig],
        features_config: Union[str, Dict, DictConfig],
        augmentation_config: Union[str, Dict, DictConfig],
        dataset_split: str,
    ):
        # these configs should already be DictConfig objects, load_config is just for safety
        dataset_config = load_config(dataset_config, "dataset_config")
        features_config = load_config(features_config, "features_config")
        self.dataset_split = dataset_split

        # Get standardized paths based on data version
        self.data_dir, metadata_path = get_data_paths(dataset_config["data_version"])
        
        # Load and filter metadata
        self.metadata = pd.read_csv(metadata_path)
        self.metadata = self.metadata[self.metadata["dataset_split"] == dataset_split]
        
        # Frame sampling configuration
        if self.dataset_split == "train":
            sampling_config = dataset_config["frame_sampling_train"]
            self.sampling_func = frame_sampling.get_sampling_function(sampling_config["method"])
            self.sampling_params = sampling_config["params"]
        else:
            # Use test sampling for validation and test splits
            sampling_config = dataset_config["frame_sampling_test"]
            self.sampling_func = frame_sampling.get_sampling_function(sampling_config["method"])
            self.sampling_params = sampling_config["params"]

        # Calculate samples per video and store all samples
        self.samples_per_video = []
        self.all_samples = []  # Store all samples for each video
        for idx in range(len(self.metadata)):
            frames = self._load_frames(idx)
            samples = self.sampling_func(
                num_frames=len(frames),
                params=self.sampling_params
            )
            self.samples_per_video.append(len(samples))
            self.all_samples.append(samples)  # Store the samples
            
        # Cumulative sum for index mapping
        self.cumsum_samples = np.cumsum([0] + self.samples_per_video)

        # Initialize feature processor
        self.feature_processor = FeatureProcessor(
            dataset_split=self.dataset_split,
            dataset_config=dataset_config,
            features_config=features_config,
            augmentation_config=augmentation_config,
            landmarks_dir=self.data_dir,
        )

    def _load_frames(self, idx: int):
        """Helper to load frames for a video."""
        idx = self.metadata.index[idx]
        landmark_path = os.path.join(self.data_dir, self.metadata.loc[idx, "filename"])
        frames = np.load(landmark_path, allow_pickle=True)
        return frames

    def __len__(self) -> int:
        return self.cumsum_samples[-1]

    def _check_landmark_config(
        self, first_key: str, second_key: str
    ) -> Tuple[str, str]:
        """To allow free order of of features inside a feature vectors"""
        if second_key in self.configuration["landmark_types"]:
            return first_key, second_key
        if first_key in self.configuration["landmark_types"]:
            return second_key, first_key

        raise Exception(
            "Error with landmark_features in dataset config, it is not specified correctly"
        )

    def __getitem__(self, idx: int):
        get_item_timing = {
            'index_calculation': 0.0,
            'frame_loading': 0.0,
            'sample_selection': 0.0,
            'metadata_lookup': 0.0,
            'feature_processing': 0.0,
            'label_creation': 0.0,
        }
        ic_start_time = time.time()
        # Find which video this index belongs to
        ## idx_position is the raw index, and can be used with df.iloc to get the ith row
        ## idx_number is the row's actual number in the index column of the df
        ## the self.metadata df is a subset of the full metadata df, but still retains the original index numbers
        ## so the index used for df.iloc and df.loc usually differ
        video_idx_position = np.searchsorted(self.cumsum_samples, idx, side='right') - 1
        video_idx_number = self.metadata.index[video_idx_position]
        # Calculate which sample number this is for this video
        sample_idx = idx - self.cumsum_samples[video_idx_position]
        get_item_timing['index_calculation'] = time.time() - ic_start_time
        
        # Load the frames
        fl_start_time = time.time()
        frames = self._load_frames(video_idx_position)
        get_item_timing['frame_loading'] = time.time() - fl_start_time
        
        # Get the pre-calculated sample for this index
        ss_start_time = time.time()
        selected_indices = self.all_samples[video_idx_position][sample_idx]
        get_item_timing['sample_selection'] = time.time() - ss_start_time

        # Get the metadata row
        ml_start_time = time.time()
        metadata_row = self.metadata.iloc[video_idx_position]
        get_item_timing['metadata_lookup'] = time.time() - ml_start_time

        # Process frames using feature processor
        fp_start_time = time.time()
        features = self.feature_processor.process_frames(frames, selected_indices, metadata_row, self.dataset_split)
        get_item_timing['feature_processing'] = time.time() - fp_start_time

        # Get label
        lc_start_time = time.time()
        label = torch.tensor([self.metadata.loc[video_idx_number, "label_encoded"]], dtype=torch.int64)
        get_item_timing['label_creation'] = time.time() - lc_start_time

        return features, label, get_item_timing
