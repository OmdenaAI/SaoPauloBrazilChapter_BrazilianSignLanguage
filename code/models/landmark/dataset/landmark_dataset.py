from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import torch
from models.landmark.utils.utils import load_config
from typing import Dict, List, Union, Callable, Tuple
from models.landmark.utils.utils import load_obj
from models.landmark.utils.path_utils import get_data_paths
from functools import partial
import os
from omegaconf import DictConfig


def uniform_intervals(start: int, end: int, interval: int):
    return list(range(start, end + 1, interval))


def random_timestamps(start: int, end: int, interval: int):
    return sorted(np.random.randint(start, end + 1, size=interval).tolist())


def uniform_timestamps(start: int, end: int, interval: int) -> List[int]:
    return list(np.linspace(start, end, num=interval, dtype=int))


INTERVAL_FUNCTIONS = {
    "uniform_intervals": uniform_intervals,
    "random_timestamps": random_timestamps,
    "uniform_timestamps": uniform_timestamps,
}


def select_frame_indices_by_time_interval(
    timestamps_ms: List[int], interval_fn: Callable[[int, int], List[int]]
) -> List[int]:
    """
    Selects frame indices based on desired timestamps generated by `interval_fn`.

    Parameters:
    -----------
    timestamps_ms : List[int]
        Sorted list of frame timestamps in milliseconds.

    interval_fn : Callable
        A function that takes (start_time, end_time) and returns a list of desired timestamps (ms).

    Returns:
    --------
    List[int] : Indices of frames closest to the desired timestamps.
    """
    timestamps = np.array(timestamps_ms)
    start_time, end_time = timestamps[0], timestamps[-1]
    target_times = interval_fn(int(start_time), int(end_time))

    selected_indices = []
    for target in target_times:
        closest_idx = np.argmin(np.abs(timestamps - target))
        if (
            not selected_indices or closest_idx != selected_indices[-1]
        ):  # avoid duplicates
            selected_indices.append(closest_idx)

    return selected_indices


class LandmarkFeatureTorchJoiner:
    def forward(self, landmark_features: Dict):
        feature_vector = []
        for landmark_type in landmark_features.keys():
            feature_vector.extend(landmark_features[landmark_type])
        return torch.tensor(feature_vector, dtype=torch.float)


class LandmarkDataset(Dataset):
    def __init__(
        self,
        dataset_config: Union[str, Dict, DictConfig],
        features_config: Union[str, Dict, DictConfig],
        augmentation_config: Union[str, Dict, DictConfig],
        dataset_split: str,
    ):
        config = load_config(dataset_config, "dataset_config")
        features_config = load_config(features_config, "features_config")

        # Get standardized paths based on data version
        self.data_dir, metadata_path = get_data_paths(config["data_version"])
        
        # Load and filter metadata
        self.data = pd.read_csv(metadata_path)
        self.data = self.data[self.data["dataset_split"] == dataset_split]
        
        self.augmentations = (
            [
                {
                    "augmentation": load_obj(augmentation["class_name"])(
                        **augmentation["params"]
                    ),
                    "p": augmentation["p"],
                }
                for _, augmentation in augmentation_config[dataset_split].items()
            ]
            if augmentation_config[dataset_split] is not None
            else []
        )

        self.frame_interval_fn = partial(
            INTERVAL_FUNCTIONS[config["frame_interval_fn"]], interval=config["interval"]
        )

        self.configuration = {
            "landmark_types": config["landmark_types"],
            "features": list(features_config.keys()),
            "ordering": config["ordering"],
        }

        self.estimators = {
            name: {
                "estimator": load_obj(estimator_params["class_name"])(
                    estimator_params["hand"],
                    estimator_params["pose"],
                ),
                "mode": estimator_params["mode"],
                "computation_type": estimator_params["computation_type"],
            }
            for name, estimator_params in features_config.items()
        }

    def __len__(self) -> int:
        return self.data.shape[0]

    def _check_landmark_config(
        self, first_key: str, second_key: str
    ) -> Tuple[str, str]:
        """To allow free order of of features inside a feature vectors"""

        if second_key in self.configuration["landmark_types"]:
            return first_key, second_key
        if first_key in self.configuration["landmark_types"]:
            return second_key, first_key

        raise Exception(
            "Error with landmark_features in dataset config, it is not specified correctly"
        )

    def __getitem__(self, idx: int):
        idx = self.data.index[idx]
        landmark_path = os.path.join(self.data_dir, self.data.loc[idx, "filename"])
        label = torch.tensor([self.data.loc[idx, "label_encoded"]], dtype=torch.int64)
        preprocessed_first_index = self.data.loc[idx, "start_frame"]
        preprocessed_last_index = self.data.loc[idx, "end_frame"]
        frames = np.load(landmark_path, allow_pickle=True)
        frames = frames[preprocessed_first_index:preprocessed_last_index]

        # Get timestamps and select relevant frame indices
        timestamps = [f["timestamp_ms"] for f in frames]
        selected_indices = select_frame_indices_by_time_interval(
            timestamps, self.frame_interval_fn
        )

        # Compute features
        all_features = []
        for indx, i in enumerate(selected_indices):
            frame = frames[i]
            frame = {
                f"{key}_landmarks": frame[f"{key}_landmarks"].landmark
                if frame[f"{key}_landmarks"] is not None
                else None
                for key in self.configuration["landmark_types"]
            }

            for aug in self.augmentations:
                if np.random.uniform() <= aug["p"]:
                    frame = aug["augmentation"](frame)
            features = {}

            prev_frame = {
                f"{key}_landmarks": None for key in self.configuration["landmark_types"]
            }
            if indx > 0:
                prev_frame = frames[selected_indices[indx - 1]]
                prev_frame = {
                    f"{key}_landmarks": prev_frame[f"{key}_landmarks"].landmark
                    if prev_frame[f"{key}_landmarks"] is not None
                    else None
                    for key in self.configuration["landmark_types"]
                }

            for first_key in self.configuration[self.configuration["ordering"][0]]:
                for second_key in self.configuration[self.configuration["ordering"][1]]:
                    feature_type, landmark_type = self._check_landmark_config(
                        first_key, second_key
                    )
                    if feature_type == "differences":
                        features[f"{feature_type}/{landmark_type}"] = self.estimators[
                            feature_type
                        ]["estimator"].compute(
                            prev_frame[f"{landmark_type}_landmarks"],
                            frame[f"{landmark_type}_landmarks"],
                            landmark_type=landmark_type.split("_")[-1],
                            mode=self.estimators[feature_type]["mode"],
                            computation_type=self.estimators[feature_type][
                                "computation_type"
                            ],
                        )
                    else:
                        features[f"{feature_type}/{landmark_type}"] = self.estimators[
                            feature_type
                        ]["estimator"].compute(
                            frame[f"{landmark_type}_landmarks"],
                            landmark_type=landmark_type.split("_")[-1],
                            mode=self.estimators[feature_type]["mode"],
                            computation_type=self.estimators[feature_type][
                                "computation_type"
                            ],
                        )
            # landmark validness features
            features["validness"] = [
                int(frame[f"{key}_landmarks"] is not None)
                for key in self.configuration["landmark_types"]
            ]
            feature_vector = np.concatenate(list(features.values()), axis=None)

            all_features.append(torch.tensor(feature_vector, dtype=torch.float))

        return torch.stack(all_features), label
