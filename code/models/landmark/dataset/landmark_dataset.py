from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import torch
from models.landmark.utils.utils import load_config, load_obj
from typing import Dict, List, Union, Callable, Tuple
from models.landmark.utils.utils import load_obj
from models.landmark.utils.path_utils import get_data_paths
from functools import partial
import os
from omegaconf import DictConfig
import os
from . import frame_sampling


def uniform_intervals(start: int, end: int, interval: int):
    return list(range(start, end + 1, interval))


def random_timestamps(start: int, end: int, interval: int):
    return sorted(np.random.randint(start, end + 1, size=interval).tolist())


def uniform_timestamps(start: int, end: int, interval: int) -> List[int]:
    return list(np.linspace(start, end, num=interval, dtype=int))


INTERVAL_FUNCTIONS = {
    "uniform_intervals": uniform_intervals,
    "random_timestamps": random_timestamps,
    "uniform_timestamps": uniform_timestamps,
}


def select_frame_indices_by_time_interval(
    timestamps_ms: List[int], 
    interval_fn: Callable[[int, int], List[int]],
    min_frames_between_samples: int = None
) -> List[int]:
    """
    Selects frame indices based on desired timestamps generated by `interval_fn`.

    Parameters:
    -----------
    timestamps_ms : List[int]
        Sorted list of frame timestamps in milliseconds.
    interval_fn : Callable
        A function that takes (start_time, end_time) and returns a list of desired timestamps (ms).
    min_frames_between_samples : int, optional
        Minimum number of frames required between samples. Used to validate sampling.

    Returns:
    --------
    List[int] : Indices of frames closest to the desired timestamps.
    """
    timestamps = np.array(timestamps_ms)
    start_time, end_time = timestamps[0], timestamps[-1]
    target_times = interval_fn(int(start_time), int(end_time))

    selected_indices = []
    for target in target_times:
        closest_idx = np.argmin(np.abs(timestamps - target))
        
        # Check if this index maintains minimum frame separation
        if min_frames_between_samples is not None and selected_indices:
            if closest_idx - selected_indices[-1] < min_frames_between_samples:
                continue
                
        if not selected_indices or closest_idx != selected_indices[-1]:  # avoid duplicates
            selected_indices.append(closest_idx)

    return selected_indices


class LandmarkFeatureTorchJoiner:
    def forward(self, landmark_features: Dict):
        feature_vector = []
        for landmark_type in landmark_features.keys():
            feature_vector.extend(landmark_features[landmark_type])
        return torch.tensor(feature_vector, dtype=torch.float)


class LandmarkDataset(Dataset):
    def __init__(
        self,
        dataset_config: Union[str, Dict, DictConfig],
        features_config: Union[str, Dict, DictConfig],
        augmentation_config: Union[str, Dict, DictConfig],
        dataset_split: str,
    ):
        config = load_config(dataset_config, "dataset_config")
        features_config = load_config(features_config, "features_config")

        # Get standardized paths based on data version
        self.data_dir, metadata_path = get_data_paths(config["data_version"])
        
        # Load and filter metadata
        self.data = pd.read_csv(metadata_path)
        self.data = self.data[self.data["dataset_split"] == dataset_split]
        
        self.augmentations = (
            [
                {
                    "augmentation": load_obj(augmentation["class_name"])(
                        **augmentation["params"]
                    ),
                    "p": augmentation["p"],
                }
                for _, augmentation in augmentation_config[dataset_split].items()
            ]
            if augmentation_config[dataset_split] is not None
            else []
        )

        # Frame sampling configuration
        if dataset_split == "train":
            sampling_config = config["frame_sampling_train"]
            self.sampling_func = frame_sampling.get_sampling_function(sampling_config["method"])
            self.sampling_params = sampling_config["params"]
        else:
            # Use test sampling for validation and test splits
            sampling_config = config["frame_sampling_test"]
            self.sampling_func = frame_sampling.get_sampling_function(sampling_config["method"])
            self.sampling_params = sampling_config["params"]

        # Calculate samples per video based on sampling method
        self.samples_per_video = []
        for idx in range(len(self.data)):
            frames = self._load_frames(idx)
            samples = self.sampling_func(
                num_frames=len(frames),
                params=self.sampling_params
            )
            self.samples_per_video.append(len(samples))
            
        # Cumulative sum for index mapping
        self.cumsum_samples = np.cumsum([0] + self.samples_per_video)

        self.configuration = {
            "landmark_types": config["landmark_types"],
            "features": list(features_config.keys()),
            "ordering": config["ordering"],
        }

        self.estimators = {
            name: {
                "estimator": load_obj(estimator_params["class_name"])(
                    estimator_params["hand"],
                    estimator_params["pose"],
                ),
                "mode": estimator_params["mode"],
                "computation_type": estimator_params["computation_type"],
            }
            for name, estimator_params in features_config.items()
            # this was empty in Ana's code too
        }

    def _load_frames(self, idx: int):
        """Helper to load frames for a video."""
        idx = self.data.index[idx]
        landmark_path = os.path.join(self.data_dir, self.data.loc[idx, "filename"])
        preprocessed_first_index = self.data.loc[idx, "start_frame"]
        preprocessed_last_index = self.data.loc[idx, "end_frame"]
        frames = np.load(landmark_path, allow_pickle=True)
        return frames[preprocessed_first_index:preprocessed_last_index]

    def __len__(self) -> int:
        return self.cumsum_samples[-1]

    def _check_landmark_config(
        self, first_key: str, second_key: str
    ) -> Tuple[str, str]:
        """To allow free order of of features inside a feature vectors"""
        if second_key in self.configuration["landmark_types"]:
            return first_key, second_key
        if first_key in self.configuration["landmark_types"]:
            return second_key, first_key

        raise Exception(
            "Error with landmark_features in dataset config, it is not specified correctly"
        )

    def __getitem__(self, idx: int):
        # Find which video this index belongs to
        video_idx = np.searchsorted(self.cumsum_samples, idx, side='right') - 1
        # Calculate which sample number this is for this video
        sample_idx = idx - self.cumsum_samples[video_idx]
        
        # Load the frames
        frames = self._load_frames(video_idx)
        
        # Get frame indices using configured sampling method
        all_samples = self.sampling_func(
            num_frames=len(frames),
            params=self.sampling_params
        )
        
        # Get the specific sample for this index
        selected_indices = all_samples[sample_idx]
        
        # Process each sample
        all_features = []
        for indx, i in enumerate(selected_indices):
            frame = frames[i]
            frame = {
                f"{key}_landmarks": frame[f"{key}_landmarks"].landmark
                if frame[f"{key}_landmarks"] is not None
                else None
                for key in self.configuration["landmark_types"]
            }

            for aug in self.augmentations:
                if np.random.uniform() <= aug["p"]:
                    frame = aug["augmentation"](frame)
            features = {}

            prev_frame = {
                f"{key}_landmarks": None for key in self.configuration["landmark_types"]
            }
            if indx > 0:
                prev_frame = frames[selected_indices[indx - 1]]
                prev_frame = {
                    f"{key}_landmarks": prev_frame[f"{key}_landmarks"].landmark
                    if prev_frame[f"{key}_landmarks"] is not None
                    else None
                    for key in self.configuration["landmark_types"]
                }

            for first_key in self.configuration[self.configuration["ordering"][0]]:
                for second_key in self.configuration[self.configuration["ordering"][1]]:
                    feature_type, landmark_type = self._check_landmark_config(
                        first_key, second_key
                    )
                    if feature_type == "differences":
                        features[f"{feature_type}/{landmark_type}"] = self.estimators[
                            feature_type
                        ]["estimator"].compute(
                            prev_frame[f"{landmark_type}_landmarks"],
                            frame[f"{landmark_type}_landmarks"],
                            landmark_type=landmark_type.split("_")[-1],
                            mode=self.estimators[feature_type]["mode"],
                            computation_type=self.estimators[feature_type][
                                "computation_type"
                            ],
                        )
                    else:
                        features[f"{feature_type}/{landmark_type}"] = self.estimators[
                            feature_type
                        ]["estimator"].compute(
                            frame[f"{landmark_type}_landmarks"],
                            landmark_type=landmark_type.split("_")[-1],
                            mode=self.estimators[feature_type]["mode"],
                            computation_type=self.estimators[feature_type][
                                "computation_type"
                            ],
                        )
            # landmark validness features
            features["validness"] = [
                int(frame[f"{key}_landmarks"] is not None)
                for key in self.configuration["landmark_types"]
            ]
            feature_vector = np.concatenate(list(features.values()), axis=None)

            all_features.append(torch.tensor(feature_vector, dtype=torch.float))

        # Get label (same for all samples from this video)
        video_idx = self.data.index[video_idx]
        label = torch.tensor([self.data.loc[video_idx, "label_encoded"]], dtype=torch.int64)

        return torch.stack(all_features), label
